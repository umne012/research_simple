def show_relation_tab():
    import streamlit as st
    import pandas as pd
    import requests
    import plotly.graph_objects as go
    from io import StringIO
    import json

    st.title("ğŸ“Œ ì—°ê´€ì–´ ë¶„ì„")

    # âœ… ì£¼ì°¨ ì„ íƒ
    weeks = {
        "3ì›” 1ì£¼ì°¨ ('25.3.1~3.7)": "2025_03w1",
        "3ì›” 2ì£¼ì°¨ ('25.3.8~3.14)": "2025_03w2",
        "3ì›” 3ì£¼ì°¨ ('25.3.15~3.21)": "2025_03w3"
    }
    selected_label = st.selectbox("ğŸ“‚ ì£¼ì°¨ ì„ íƒ", list(weeks.keys()), index=0)
    selected_week = weeks[selected_label]

    base_url = f"https://raw.githubusercontent.com/umne012/research_simple/main/{selected_week}"
    word_url = f"{base_url}/morpheme_word_count_merged.csv"
    morph_urls = [f"{base_url}/morpheme_analysis_part{i}.csv" for i in range(1, 4)]
    sentiment_url = f"{base_url}/sentiment_analysis_merged.csv"

    @st.cache_data(show_spinner=False)
    def load_data():
        word_df = pd.read_csv(word_url)
        word_data = {brand: df for brand, df in word_df.groupby("ê·¸ë£¹")}

        morph_frames = []
        for url in morph_urls:
            try:
                df = pd.read_csv(url)
                if not df.empty and all(col in df.columns for col in ["ë‹¨ì–´", "ê°ì •", "ë¬¸ì¥ID"]):
                    morph_frames.append(df)
            except Exception as e:
                st.warning(f"âš ï¸ {url} ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        if not morph_frames:
            st.error("âŒ í˜•íƒœì†Œ ë¶„ì„ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return None, None, None
        morph_df = pd.concat(morph_frames, ignore_index=True)

        try:
            sent_df = pd.read_csv(sentiment_url)
        except Exception as e:
            st.error(f"sentiment_analysis.csv ë¶ˆëŸ¬ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return None, None, None

        morph_df["ë¬¸ì¥ID"] = morph_df["ë¬¸ì¥ID"].astype(str)
        sent_df["ë¬¸ì¥ID"] = sent_df["ë¬¸ì¥ID"].astype(str)

        return word_data, morph_df, sent_df

    word_data, morph_df, sent_df = load_data()
    if word_data is None:
        return

    nodes, links, added_words = [], [], set()
    sentence_map = {}
    link_counter = {}

    def highlight_and_shorten(text, keyword):
        if keyword not in text:
            return text[:50] + "..." if len(text) > 50 else text
        idx = text.index(keyword)
        start = max(0, idx - 15)
        end = min(len(text), idx + len(keyword) + 15)
        snippet = text[start:end]
        if start > 0:
            snippet = "..." + snippet
        if end < len(text):
            snippet += "..."
        return snippet.replace(keyword, f"<b style='background:yellow'>{keyword}</b>")

    for brand, df in word_data.items():
        nodes.append({"id": brand, "group": "brand"})
        word_entries = []
        for _, row in df.iterrows():
            word = row["ë‹¨ì–´"]
            if row.get("positive", 0) > 0:
                word_entries.append((f"{word}_positive", row["positive"], "positive", word))
            if row.get("negative", 0) > 0:
                word_entries.append((f"{word}_negative", row["negative"], "negative", word))
        top_entries = sorted(word_entries, key=lambda x: x[1], reverse=True)[:10]
        for node_id, freq, sentiment, word in top_entries:
            if node_id not in added_words:
                nodes.append({"id": node_id, "group": sentiment, "freq": freq})
                added_words.add(node_id)

                match = morph_df[(morph_df["ë‹¨ì–´"] == word) & (morph_df["ê°ì •"] == sentiment)]
                matched_ids = match["ë¬¸ì¥ID"].unique()
                matched_sents = sent_df[sent_df["ë¬¸ì¥ID"].isin(matched_ids)]
                shown = []
                for _, row in matched_sents.iterrows():
                    snippet = highlight_and_shorten(str(row["ë¬¸ì¥"]), word)
                    shown.append({"ë¬¸ì¥": snippet, "ì›ë³¸ë§í¬": row["ì›ë³¸ë§í¬"], "count": freq})
                sentence_map[node_id] = shown

            links.append({"source": brand, "target": node_id})
            link_counter[node_id] = link_counter.get(node_id, 0) + 1

    nodes_json = json.dumps(nodes)
    links_json = json.dumps(links)
    sentences_json = json.dumps(sentence_map, ensure_ascii=False)

    html_template = open("network_graph.html", encoding="utf-8").read()
    st.components.v1.html(html_template, height=650)

    # âœ… ì„ ê·¸ë˜í”„ (Plotly Graph Object ë°©ì‹ìœ¼ë¡œ êµì²´)
    st.markdown("### ğŸ“Š ì¼ìë³„ ì–¸ê¸‰ëŸ‰ ì¶”ì´")
    if sent_df is not None and "ë‚ ì§œ" in sent_df.columns and "ì›ë³¸ë§í¬" in sent_df.columns and "ê·¸ë£¹" in sent_df.columns:
        mention_daily = sent_df.groupby(["ë‚ ì§œ", "ê·¸ë£¹"])["ì›ë³¸ë§í¬"].nunique().reset_index(name="ì–¸ê¸‰ëŸ‰")

        layout = go.Layout(
            plot_bgcolor="#ffffff",
            paper_bgcolor="#ffffff",
            title=dict(text="ì¼ìë³„ ë¸Œëœë“œ ì–¸ê¸‰ëŸ‰ ì¶”ì´", x=0.05, font=dict(size=18)),
            margin=dict(l=40, r=40, t=60, b=100),
            xaxis=dict(title="ë‚ ì§œ", showgrid=True, tickangle=-45),
            yaxis=dict(title="ì–¸ê¸‰ëŸ‰", showgrid=True),
            legend=dict(
                orientation="h",
                x=0.5,
                y=-0.2,
                xanchor="center",
                yanchor="top"
            )
        )

        fig = go.Figure(layout=layout)
        for group, df in mention_daily.groupby("ê·¸ë£¹"):
            fig.add_trace(go.Scatter(
                x=df["ë‚ ì§œ"],
                y=df["ì–¸ê¸‰ëŸ‰"],
                mode="lines+markers",
                name=group
            ))

        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("ğŸ“Œ ì¼ìë³„ ì–¸ê¸‰ëŸ‰ì„ ì‹œê°í™”í•˜ë ¤ë©´ sentiment_analysis.csvì— 'ë‚ ì§œ', 'ê·¸ë£¹' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
